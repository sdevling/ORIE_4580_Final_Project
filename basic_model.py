# -*- coding: utf-8 -*-
"""basic_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ms0u4aUyU1JrQYs0taSe4Mr_AOp9-YCA
"""

#Imports
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple, Optional, Protocol, Dict, Any
import numpy as np
import matplotlib.pyplot as plt

"""
Represents a single LLM query, tracking its arrival time, prefill progress,
decode progress, and timestamps for TTFT and full completion.
"""

@dataclass
class Query:
    id: int #Unique identifier for query
    arrival_time: float # Time query arrives
    L: int              # Prompt length of query
    B: int              # Max response length of query
    prefill_done: int = 0 # Number of prefill tokens completed
    decode_done: int = 0 # Number of decode tokens completed
    ttft_time: float | None = None  # Time prefill completed
    completion_time: float | None = None # Simululation time when query's last token is completed
    last_decode_finish: float | None = None # Time of last decode token

"""
Returns batch service time in seconds.
c, a are in seconds.
Based on model from Li et al.
"""
def service_time(token_load: int, c: float, a: float, b0: int) -> float:

    return c + a * max(0, token_load - b0)

# A BatchItem represents one unit of work inside a GPU batch.
# Fields:
#   - query:   the Query object this work belongs to
#   - n_tokens: number of tokens to process for this query in this batch
#   - phase:   "prefill" or "decode"
# A batch can contain multiple BatchItems (one per query during decode).
BatchItem = Tuple[Query, int, str]
Batch = List[BatchItem]


class Policy(Protocol):
    """
    Interface for all batching/scheduling policies.

    A Policy decides:
      - which queries are selected for the next GPU batch,
      - how many tokens from each query to include,
      - whether the batch is a prefill or decode batch,
      - and when the batch should be dispatched (immediately or after a delay).

    The simulator will call dispatch_batch() whenever the GPU becomes idle.
    Any class that implements this method with the same signature is
    considered a valid Policy.
    """

    def dispatch_batch(
        self,
        t: float,
        prefill_queue: List[Query],
        decode_queue: List[Query],
        *,
        c: float,
        a: float,
        b0: int,
        K: int
    ) -> Tuple[Batch, Optional[float]]:
        """
        Decide what batch to run next on the GPU.

        Parameters:
        t : float
            Current simulation time.

        prefill_queue : List[Query]
            Queries that still have prefill tokens remaining.
            Prefill can include multiple tokens from the same query.

        decode_queue : List[Query]
            Queries that have finished prefill and are awaiting decode.
            Each query can contribute at most one decode token per batch.

        c, a, b0 : floats and int
            Parameters for the service-time model:
                S(b) = c + a * max(0, b - b0)
            where b is the total number of tokens in the batch.
            Note that time units are in seconds.

        K : int
            Maximum allowed number of tokens in one batch.

        Returns:
        batch : Batch
            A list of BatchItems describing the next batch to run.

        next_wakeup : Optional[float]
            If None: dispatch immediately.
            If a future time t_future is returned: the simulator will wait
            until that time before forming the batch.
        """
        ...

class ServeToCompletionPolicy:
    """
    Baseline scheduling policy: serve one query at a time until it is finished.

    Behavior:
      - Pick the earliest-arriving query.
      - Process all of its prefill tokens before decoding.
      - Then process all decode tokens for that query.
      - Never batch across multiple queries.
      - Prefill can process up to K tokens at once.
      - Decode must process exactly 1 token at a time.
    """

    def __init__(self):
        # The query currently being served (None if GPU is idle)
        self.current_query: Optional[Query] = None

        # Phase of the current query: "prefill" or "decode"
        self.current_phase: Optional[str] = None

    def dispatch_batch(
        self, t, prefill_queue, decode_queue, *, c, a, b0, K
    ):
        """
        Select the next GPU batch for this policy.

        Parameters:
        t : float
            Current simulation time.

        prefill_queue : List[Query]
            Queries with remaining prefill tokens (prompt processing).

        decode_queue : List[Query]
            Queries with remaining decode tokens (must be 1/token per batch).

        c, a, b0 : floats
            Parameters for the service-time model:
                S(b) = c + a * max(0, b - b0)

        K : int
            Max number of tokens in a batch (only meaningful for prefill).

        Returns:
        batch : List[BatchItem]
            A batch containing ONE BatchItem: (query, n_tokens, phase).

        next_time : float
            Time at which the GPU will finish this batch (t + service_time).
        """

        # CASE 1: No queries anywhere and no active query
        # Do nothing
        if not prefill_queue and not decode_queue and self.current_query is None:
            return [], None

        # CASE 2: We do not currently have a query assigned.
        # Pick the earliest-arriving query from both queues.

        if self.current_query is None:
            available = []
            available.extend(prefill_queue)
            available.extend(decode_queue)

            if not available:
                return [], None

            # Choose earliest arrival time
            q = min(available, key=lambda x: x.arrival_time)
            self.current_query = q

            # Determine starting phase
            self.current_phase = (
                "prefill" if q.prefill_done < q.L else "decode"
            )

        # Local references
        q = self.current_query
        phase = self.current_phase

        # CASE 3: If prefill just finished, switch phase to decode.
        if phase == "prefill" and q.prefill_done >= q.L:
            phase = "decode"
            self.current_phase = phase

        # CASE 4: If decode is finished, drop this query entirely
        # and immediately pick the next one via recursion.
        if phase == "decode" and q.decode_done >= q.B:
            self.current_query = None
            self.current_phase = None
            return self.dispatch_batch(
                t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K
            )

        # CASE 5: Determine how many tokens to process for this batch.
        #  Prefill may process up to K tokens at once.
        #  Decode must only generate 1 token.

        if phase == "prefill":
            remaining = q.L - q.prefill_done
            n_tokens = min(remaining, K)      # prefill can batch
        else:  # phase == "decode"
            remaining = q.B - q.decode_done
            n_tokens = 1                      # decode always 1 token


        # CASE 6: Compute service time using token_load = n_tokens
        token_load = n_tokens
        st = service_time(token_load, c, a, b0)

        # CASE 7: Build the actual batch
        # Only one query is ever included in this policy
        batch = [(q, n_tokens, phase)]

        # GPU will finish processing this batch at time t + st
        return batch, t + st

class PrefillFirstPolicy:
    """
    Policy: Always process prefill work first.

    Behavior:
      - If any query still has prefill tokens, process them first.
      - Prefill batches can include up to K tokens.
      - Only process decode work when no prefill work remains.
      - Decode batches include exactly 1 token per query.
      - A batch may include multiple queries, but total tokens <= K.
    """

    def dispatch_batch(
        self, t, prefill_queue, decode_queue, *, c, a, b0, K
    ):
        # CASE 1: No work
        if not prefill_queue and not decode_queue:
            return [], None

        batch: Batch = []
        tokens_used = 0  # total tokens included in this batch so far

        # CASE 2: Prefill has priority
        # We collect prefill tokens from earliest-arriving queries first.
        # Prefill can take many tokens from a single query.
        if prefill_queue:
            # Serve queries in FIFO order
            for q in sorted(prefill_queue, key=lambda x: x.arrival_time):

                remaining = q.L - q.prefill_done  # remaining prefill tokens
                if remaining <= 0:
                    continue

                # Take as many tokens as possible from this query,
                # but do not exceed remaining batch capacity K.
                take = min(remaining, K - tokens_used)

                if take > 0:
                    batch.append((q, take, "prefill"))
                    tokens_used += take

                # Stop if we have filled the batch capacity
                if tokens_used >= K:
                    break

        # CASE 3: Serve decode tokens if no prefill tokens remain
        # Decode can only take 1 token per query.
        else:
            for q in sorted(decode_queue, key=lambda x: x.arrival_time):

                remaining = q.B - q.decode_done  # decode tokens left
                if remaining <= 0:
                    continue

                # Add exactly one decode token from this query
                if tokens_used < K:
                    batch.append((q, 1, "decode"))
                    tokens_used += 1

                if tokens_used >= K:
                    break

        # CASE 4: No tokens added
        if not batch:
            return [], None

        # CASE 5: Compute service time based on total token load
        token_load = sum(n for _, n, _ in batch)
        st = service_time(token_load, c, a, b0)

        # Return batch and the time when the GPU will finish processing it
        return batch, t + st

def apply_batch_completion(
    t: float,
    batch: Batch,
    prefill_queue: List[Query],
    decode_queue: List[Query],
    *,
    warmup_id: int,
    metrics: Dict[str, List[float]],
    #last_decode_event_time: Optional[float],
):


    # Process each item in the completed batch
    for q, n_tokens, phase in batch:


        # Prefill phase update
        if phase == "prefill":

            q.prefill_done += n_tokens

            # Check if prefill completed
            if q.prefill_done >= q.L and q.ttft_time is None:
                q.ttft_time = t

                if q.id >= warmup_id:
                    metrics["ttft"].append(q.ttft_time - q.arrival_time)

                if q not in decode_queue:
                    decode_queue.append(q)

        # Decode phase update
        elif phase == "decode":

            q.decode_done += n_tokens

            # Compute per-token TBT
            if q.id >= warmup_id:
                if q.last_decode_finish is not None:
                    metrics["tbt"].append(t - q.last_decode_finish)
                    metrics["tbt_time"].append(t)
                q.last_decode_finish = t

            # If decode finishes, record completion time
            if q.decode_done >= q.B and q.completion_time is None:
                q.completion_time = t

                if q.id >= warmup_id:
                    metrics["completion"].append(q.completion_time - q.arrival_time)



    # Clean up queues
    prefill_queue[:] = [q for q in prefill_queue if q.prefill_done < q.L]

    decode_queue[:] = [
        q for q in decode_queue
        if q.prefill_done >= q.L and q.decode_done < q.B
    ]

def simulate(
    policy: Policy,
    *,
    n_queries: int = 4000,      # total number of queries the simulator will process
    warmup: int = 500,          # number of initial queries ignored in metrics
    lam: float = 1.1,         # average number query arrivals (Poisson process)
    L: int = 64,                # prefill tokens per query
    B: int = 32,                # decode tokens per query
    c_ms: float = 40.0,         # service model constant term (ms)
    a_ms: float = 0.3,          # service model slope term (ms/token)
    b0: int = 64,               # breakpoint for service model
    K: int = 128,               # batch token capacity
    seed: int = 0,
) -> Dict[str, Any]:

    rng = np.random.default_rng(seed)

    # Convert ms to seconds for service model
    c = c_ms / 1000.0
    a = a_ms / 1000.0

    # Current simulation time
    t = 0.0

    # Time of next arrival event
    next_arrival = rng.exponential(1 / lam)

    # GPU state
    gpu_busy_until: Optional[float] = None
    active_batch: Batch = []

    # Queues
    prefill_queue: List[Query] = []
    decode_queue: List[Query] = []
    all_queries: List[Query] = []

    # Metrics
    metrics = {
        "ttft": [],
        "ttft_time": [],
        "tbt": [],
        "tbt_time": [],
        "completion": [],
        "first_decode_times": []
    }


    # Completions counting for steady-state
    completed_after_warmup = 0
    target_completed = n_queries - warmup

    # Helper: dispatch a new batch if GPU is idle
    def try_dispatch():
        nonlocal active_batch, gpu_busy_until
        active_batch, gpu_busy_until = policy.dispatch_batch(
            t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K
        )

    # Main event loop
    while completed_after_warmup < target_completed:

        # Next arrival vs next GPU completion
        candidate_times = [next_arrival]
        if gpu_busy_until is not None:
            candidate_times.append(gpu_busy_until)

        # Advance simulation time
        t_next = min(candidate_times)
        t = t_next

        # EVENT TYPE 1: arrival
        if t == next_arrival:
            q = Query(id=len(all_queries), arrival_time=t, L=L, B=B)
            all_queries.append(q)
            prefill_queue.append(q)

            next_arrival = t + rng.exponential(1 / lam)

            if gpu_busy_until is None:
                try_dispatch()

        # EVENT TYPE 2: Batch Completion
        else:
            gpu_busy_until = None

            before = len(metrics["completion"])

            apply_batch_completion(
                t,
                active_batch,
                prefill_queue,
                decode_queue,
                warmup_id=warmup,
                metrics=metrics,
            )

            after = len(metrics["completion"])
            completed_after_warmup += (after - before)

            try_dispatch()

    # Throughput estimation
    post = [q for q in all_queries if q.id >= warmup and q.completion_time is not None]
    post.sort(key=lambda q: q.completion_time)

    if len(post) >= 2:
        elapsed = post[-1].completion_time - post[0].arrival_time
        throughput = len(post) / elapsed if elapsed > 0 else float("nan")
    else:
        throughput = float("nan")


    # Return metrics
    return {
        "throughput": throughput,
        "ttft": np.array(metrics["ttft"]),
        "ttft_time": np.array(metrics["ttft_time"]),
        "tbt": np.array(metrics["tbt"]),
        "tbt_time": np.array(metrics["tbt_time"]),
        "completion": np.array(metrics["completion"]),
    }

def run_demo():
    """
    Run a simple comparative demo between two batching policies:
      • Serve-to-Completion
      • Prefill-First
    """

    # Two policies to compare
    baseline = ServeToCompletionPolicy()
    prefill_first = PrefillFirstPolicy()

    # Run simulation under each policy
    # Uses the same seed for both simulations
    r1 = simulate(baseline, seed=1)
    r2 = simulate(prefill_first, seed=1)

    # Print throughput comparison
    print("Serve-to-completion throughput:", r1["throughput"])
    print("Prefill-first throughput:", r2["throughput"])

    # Print TTFT
    print("Mean TTFT baseline:", r1["ttft"].mean())
    print("Mean TTFT prefill-first:", r2["ttft"].mean())

    # Print token-level TBT
    # TBT may be empty depending on workload,so guard with a length check.
    print("Mean token-level TBT baseline:",
          r1["tbt"].mean() if len(r1["tbt"]) else np.nan)

    print("Mean token-level TBT prefill-first:",
          r2["tbt"].mean() if len(r2["tbt"]) else np.nan)


    # Return raw results for further analysis and plotting
    return r1, r2

def plot_demo(r1, r2):
    # Plot smoothed TTFT
    plt.figure(figsize=(8,5))
    plt.plot(moving_avg(r1["ttft"]), label="TTFT baseline", alpha=0.8)
    plt.plot(moving_avg(r2["ttft"]), label="TTFT prefill-first", alpha=0.8)
    plt.xlabel("Query index (smoothed)")
    plt.ylabel("TTFT (s)")
    plt.title("TTFT over time (smoothed)")
    plt.legend()
    plt.show()

    # Plot smoothed baseline TBT
    smoothed_baseline_tbt = moving_avg(r1["tbt"])
    smoothed_baseline_time = r1["tbt_time"][:len(smoothed_baseline_tbt)]

    plt.figure(figsize=(8,5))
    plt.plot(smoothed_baseline_time, smoothed_baseline_tbt,
             label="TBT baseline", alpha=0.8)
    plt.xlabel("Time (s)")
    plt.ylabel("TBT (s)")
    plt.title("TBT over time — baseline policy (smoothed)")
    plt.legend()
    plt.show()

    # Plot smoothed prefill-first TBT
    smoothed_prefill_tbt = moving_avg(r2["tbt"])
    smoothed_prefill_time = r2["tbt_time"][:len(smoothed_prefill_tbt)]

    plt.figure(figsize=(8,5))
    plt.plot(smoothed_prefill_time, smoothed_prefill_tbt,
             label="TBT prefill-first", alpha=0.8, color="orange")
    plt.xlabel("Time (s)")
    plt.ylabel("TBT (s)")
    plt.title("TBT over time — prefill-first policy (smoothed)")
    plt.legend()
    plt.show()


def plot_demo_hist(r1, r2, bin_width=100.0):
    """
    Plot histograms of mean TTFT and mean TBT within fixed time bins.
    """

    # Helper: compute histogram of means
    def mean_hist(times, values, bin_width):
        if len(times) == 0:
            return np.array([]), np.array([])

        t_min = times.min()
        t_max = times.max()

        bins = np.arange(t_min, t_max + bin_width, bin_width)

        # Digitize into bins
        idx = np.digitize(times, bins) - 1

        bin_means = []
        bin_centers = []

        for b in range(len(bins) - 1):
            mask = (idx == b)
            if mask.any():
                bin_means.append(values[mask].mean())
                bin_centers.append((bins[b] + bins[b+1]) / 2)

        return np.array(bin_centers), np.array(bin_means)


    # TTFT histograms (by arrival order, not time)
    plt.figure(figsize=(8,5))

    # Treat TTFT order index as "time"
    ttft_t1 = np.arange(len(r1["ttft"]))
    ttft_t2 = np.arange(len(r2["ttft"]))

    c1, m1 = mean_hist(ttft_t1, r1["ttft"], bin_width)
    c2, m2 = mean_hist(ttft_t2, r2["ttft"], bin_width)

    plt.bar(c1 - bin_width*0.15, m1, width=bin_width*0.3, label="TTFT baseline")
    plt.bar(c2 + bin_width*0.15, m2, width=bin_width*0.3, label="TTFT prefill-first")

    plt.xlabel("Query index (binned)")
    plt.ylabel("Mean TTFT (s)")
    plt.title("Mean TTFT in fixed index bins")
    plt.legend()
    plt.show()


    # TBT histograms (time-based)
    plt.figure(figsize=(8,5))

    c1, m1 = mean_hist(r1["tbt_time"], r1["tbt"], bin_width)
    c2, m2 = mean_hist(r2["tbt_time"], r2["tbt"], bin_width)

    plt.bar(c1 - bin_width*0.15, m1, width=bin_width*0.3, label="TBT baseline")
    plt.bar(c2 + bin_width*0.15, m2, width=bin_width*0.3, label="TBT prefill-first")

    plt.xlabel("Time (s, binned)")
    plt.ylabel("Mean TBT (s)")
    plt.title("Mean TBT in time bins")
    plt.legend()
    plt.show()

if __name__ == "__main__":
    r1, r2 = run_demo()
    #plot_demo(r1, r2)
    plot_demo_hist(r1,r2)

def run_phase_validation():
    """
    Validation attempt (Idea 2 from project spec):

      1. Sweep several input rates λ (queries / second).
      2. For each λ, run the simulator to steady state.
      3. Estimate the average number of queries in system via Little's law:
             L_hat(λ) ≈ λ_eff(λ) * E[W(λ)],
         where:
             λ_eff  = measured throughput from the simulator
             E[W]   = mean completion time (arrival → completion).
      4. Plot L_hat as a function of λ and look for a "phase transition"
         where L_hat grows rapidly as load increases.

    """

    policy = ServeToCompletionPolicy()
    lam_values = [0.2,0.3 ,0.5,0.6,0.75, 1.0,1.5]

    sim_lambda = []
    sim_L = []

    for lam in lam_values:
        results = simulate(
            policy,
            lam=lam,
            n_queries=3000,
            warmup=500,
            seed=123 + int(lam * 10),
        )

        if len(results["completion"]) > 0:
            W_hat = float(results["completion"].mean())
        else:
            W_hat = float("nan")

        lambda_eff = results["throughput"]

        if not np.isnan(W_hat) and lambda_eff > 0:
            L_hat = lambda_eff * W_hat
        else:
            L_hat = float("nan")

        sim_lambda.append(lam)
        sim_L.append(L_hat)

    print("\n Validation: Phase transition in L(λ) ")
    print("λ    L_hat")
    for lam, Lh in zip(sim_lambda, sim_L):
        print(f"{lam:5.2f}   {Lh:9.3f}")

    plt.figure(figsize=(7, 4))
    plt.plot(sim_lambda, sim_L, marker="o")
    plt.xlabel("Input rate λ (queries / second)")
    plt.ylabel("Average # of queries in system (L_hat)")
    plt.title("Idea 2 validation: estimated L(λ) and phase transition")
    plt.tight_layout()
    plt.show()

    return {
        "lambda": np.array(sim_lambda),
        "L_hat": np.array(sim_L),
    }


def run_phase_transition_validation():
    print("\nRunning Idea 2 (phase transition) validation...\n")
    res = run_phase_validation()
    print("\nIdea 2 validation finished.")
    return res