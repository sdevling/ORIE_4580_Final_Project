# -*- coding: utf-8 -*-
"""Batching_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RaKYo_aOUeNMCf_ip9_0KS2SN4j552je
"""

#Imports
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple, Optional, Protocol, Dict, Any
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""
Represents a single LLM query, tracking its arrival time, prefill progress,
decode progress, and timestamps for TTFT and full completion.
"""

@dataclass
class Query:
    id: int #Unique identifier for query
    arrival_time: float # Time query arrives
    L: int              # Prompt length of query
    B: int              # Max response length of query
    prefill_done: int = 0 # Number of prefill tokens completed
    decode_done: int = 0 # Number of decode tokens completed
    ttft_time: float | None = None  # Time prefill completed
    completion_time: float | None = None # Simululation time when query's last token is completed
    last_decode_finish: float | None = None # Time of last decode token

"""
Returns batch service time in seconds.
c, a are in seconds.
Based on model from Li et al.
"""
def service_time(token_load: int, c: float, a: float, b0: int) -> float:

    return c + a * max(0, token_load - b0)

# A BatchItem represents one unit of work inside a GPU batch.
# Fields:
#   - query:   the Query object this work belongs to
#   - n_tokens: number of tokens to process for this query in this batch
#   - phase:   "prefill" or "decode"
# A batch can contain multiple BatchItems (one per query during decode).
BatchItem = Tuple[Query, int, str]
Batch = List[BatchItem]


class Policy(Protocol):
    """
    Interface for all batching/scheduling policies.

    A Policy decides:
      - which queries are selected for the next GPU batch,
      - how many tokens from each query to include,
      - whether the batch is a prefill or decode batch,
      - and when the batch should be dispatched (immediately or after a delay).

    The simulator will call dispatch_batch() whenever the GPU becomes idle.
    Any class that implements this method with the same signature is
    considered a valid Policy.
    """

    def dispatch_batch(
        self,
        t: float,
        prefill_queue: List[Query],
        decode_queue: List[Query],
        *,
        c: float,
        a: float,
        b0: int,
        K: int
    ) -> Tuple[Batch, Optional[float]]:
        """
        Decide what batch to run next on the GPU.

        Parameters:
        t : float
            Current simulation time.

        prefill_queue : List[Query]
            Queries that still have prefill tokens remaining.
            Prefill can include multiple tokens from the same query.

        decode_queue : List[Query]
            Queries that have finished prefill and are awaiting decode.
            Each query can contribute at most one decode token per batch.

        c, a, b0 : floats and int
            Parameters for the service-time model:
                S(b) = c + a * max(0, b - b0)
            where b is the total number of tokens in the batch.
            Note that time units are in seconds.

        K : int
            Maximum allowed number of tokens in one batch.

        Returns:
        batch : Batch
            A list of BatchItems describing the next batch to run.

        next_wakeup : Optional[float]
            If None: dispatch immediately.
            If a future time t_future is returned: the simulator will wait
            until that time before forming the batch.
        """
        ...

class DynamicBatchPolicy:
    def dispatch_batch(self, t, prefill_queue, decode_queue, *, c, a, b0, K):
        batch: Batch = []
        remaining = K

        # pick only a few queries instead of filling K
        for q in prefill_queue[:3]:  # limit number of queries
            tokens = min(q.remaining_prefill(), remaining)
            batch.append((q, tokens, "prefill"))
            remaining -= tokens

        # decode tokens if space remains
        if remaining > 0:
            for q in decode_queue[:2]:
                batch.append((q, 1, "decode"))
                remaining -= 1

        return batch, None

class ServeToCompletionPolicy:
    """
    Baseline scheduling policy: serve one query at a time until it is finished.

    Behavior:
      - Pick the earliest-arriving query.
      - Process all of its prefill tokens before decoding.
      - Then process all decode tokens for that query.
      - Never batch across multiple queries.
      - Prefill can process up to K tokens at once.
      - Decode must process exactly 1 token at a time.
    """

    def __init__(self):
        # The query currently being served (None if GPU is idle)
        self.current_query: Optional[Query] = None

        # Phase of the current query: "prefill" or "decode"
        self.current_phase: Optional[str] = None

    def dispatch_batch(
        self, t, prefill_queue, decode_queue, *, c, a, b0, K
    ):
        """
        Select the next GPU batch for this policy.

        Parameters:
        t : float
            Current simulation time.

        prefill_queue : List[Query]
            Queries with remaining prefill tokens (prompt processing).

        decode_queue : List[Query]
            Queries with remaining decode tokens (must be 1/token per batch).

        c, a, b0 : floats
            Parameters for the service-time model:
                S(b) = c + a * max(0, b - b0)

        K : int
            Max number of tokens in a batch (only meaningful for prefill).

        Returns:
        batch : List[BatchItem]
            A batch containing ONE BatchItem: (query, n_tokens, phase).

        next_time : float
            Time at which the GPU will finish this batch (t + service_time).
        """

        # CASE 1: No queries anywhere and no active query
        # Do nothing
        if not prefill_queue and not decode_queue and self.current_query is None:
            return [], None

        # CASE 2: We do not currently have a query assigned.
        # Pick the earliest-arriving query from both queues.

        if self.current_query is None:
            available = []
            available.extend(prefill_queue)
            available.extend(decode_queue)

            if not available:
                return [], None

            # Choose earliest arrival time
            q = min(available, key=lambda x: x.arrival_time)
            self.current_query = q

            # Determine starting phase
            self.current_phase = (
                "prefill" if q.prefill_done < q.L else "decode"
            )

        # Local references
        q = self.current_query
        phase = self.current_phase

        # CASE 3: If prefill just finished, switch phase to decode.
        if phase == "prefill" and q.prefill_done >= q.L:
            phase = "decode"
            self.current_phase = phase

        # CASE 4: If decode is finished, drop this query entirely
        # and immediately pick the next one via recursion.
        if phase == "decode" and q.decode_done >= q.B:
            self.current_query = None
            self.current_phase = None
            return self.dispatch_batch(
                t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K
            )

        # CASE 5: Determine how many tokens to process for this batch.
        #  Prefill may process up to K tokens at once.
        #  Decode must only generate 1 token.

        if phase == "prefill":
            remaining = q.L - q.prefill_done
            n_tokens = min(remaining, K)      # prefill can batch
        else:  # phase == "decode"
            remaining = q.B - q.decode_done
            n_tokens = 1                      # decode always 1 token


        # CASE 6: Compute service time using token_load = n_tokens
        token_load = n_tokens
        st = service_time(token_load, c, a, b0)

        # CASE 7: Build the actual batch
        # Only one query is ever included in this policy
        batch = [(q, n_tokens, phase)]

        # GPU will finish processing this batch at time t + st
        return batch, t + st

class PrefillFirstPolicy:
    """
    Policy: Always process prefill work first.

    Behavior:
      - If any query still has prefill tokens, process them first.
      - Prefill batches can include up to K tokens.
      - Only process decode work when no prefill work remains.
      - Decode batches include exactly 1 token per query.
      - A batch may include multiple queries, but total tokens <= K.
    """

    def dispatch_batch(
        self, t, prefill_queue, decode_queue, *, c, a, b0, K
    ):
        # CASE 1: No work
        if not prefill_queue and not decode_queue:
            return [], None

        batch: Batch = []
        tokens_used = 0  # total tokens included in this batch so far

        # CASE 2: Prefill has priority
        # We collect prefill tokens from earliest-arriving queries first.
        # Prefill can take many tokens from a single query.
        if prefill_queue:
            # Serve queries in FIFO order
            for q in sorted(prefill_queue, key=lambda x: x.arrival_time):

                remaining = q.L - q.prefill_done  # remaining prefill tokens
                if remaining <= 0:
                    continue

                # Take as many tokens as possible from this query,
                # but do not exceed remaining batch capacity K.
                take = min(remaining, K - tokens_used)

                if take > 0:
                    batch.append((q, take, "prefill"))
                    tokens_used += take

                # Stop if we have filled the batch capacity
                if tokens_used >= K:
                    break

        # CASE 3: Serve decode tokens if no prefill tokens remain
        # Decode can only take 1 token per query.
        else:
            for q in sorted(decode_queue, key=lambda x: x.arrival_time):

                remaining = q.B - q.decode_done  # decode tokens left
                if remaining <= 0:
                    continue

                # Add exactly one decode token from this query
                if tokens_used < K:
                    batch.append((q, 1, "decode"))
                    tokens_used += 1

                if tokens_used >= K:
                    break

        # CASE 4: No tokens added
        if not batch:
            return [], None

        # CASE 5: Compute service time based on total token load
        token_load = sum(n for _, n, _ in batch)
        st = service_time(token_load, c, a, b0)

        # Return batch and the time when the GPU will finish processing it
        return batch, t + st

class HybridSchedulerPolicy:
    """
    Hybrid scheduling policy:

    - Queries are grouped into batches of at most Q queries.
    - Once a batch starts, it runs to completion (prefill + decode)
      before the next batch begins.
    - Within a batch, scheduling is prefill-first.
    """

    def __init__(self, Q: int):
        self.Q = Q

        # Active batch state
        self.active_batch_queries: List[Query] = []
        self.batch_active: bool = False

    def dispatch_batch(
        self,
        t: float,
        prefill_queue: List[Query],
        decode_queue: List[Query],
        *,
        c: float,
        a: float,
        b0: int,
        K: int
    ):

        # CASE 1: No active batch → try to form a new batch
        if not self.batch_active:

            if not prefill_queue and not decode_queue:
                return [], None

            # Pick up to Q earliest-arriving queries
            available = prefill_queue + decode_queue
            available.sort(key=lambda q: q.arrival_time)

            self.active_batch_queries = available[:self.Q]
            self.batch_active = True


        # CASE 2: Run one step of the active batch (prefill-first)
        batch: Batch = []
        tokens_used = 0

        # Separate active batch queries by phase
        active_prefill = [
            q for q in self.active_batch_queries
            if q.prefill_done < q.L
        ]
        active_decode = [
            q for q in self.active_batch_queries
            if q.prefill_done >= q.L and q.decode_done < q.B
        ]

        # Prefill-first
        if active_prefill:
            for q in sorted(active_prefill, key=lambda q: q.arrival_time):
                remaining = q.L - q.prefill_done
                take = min(remaining, K - tokens_used)

                if take > 0:
                    batch.append((q, take, "prefill"))
                    tokens_used += take

                if tokens_used >= K:
                    break

        # Decode only if no prefill remains
        else:
            for q in sorted(active_decode, key=lambda q: q.arrival_time):
                if tokens_used < K:
                    batch.append((q, 1, "decode"))
                    tokens_used += 1
                if tokens_used >= K:
                    break


        # CASE 3: Batch finished → clear active batch
        if not batch:
            # All queries in this batch are complete
            self.active_batch_queries = []
            self.batch_active = False
            return [], None


        # CASE 4: Compute service time
        token_load = sum(n for _, n, _ in batch)
        st = service_time(token_load, c, a, b0)

        return batch, t + st

def apply_batch_completion(
    t: float,
    batch: Batch,
    prefill_queue: List[Query],
    decode_queue: List[Query],
    *,
    warmup_id: int,
    metrics: Dict[str, List[float]],
    #last_decode_event_time: Optional[float],
):


    # Process each item in the completed batch
    for q, n_tokens, phase in batch:


        # Prefill phase update
        if phase == "prefill":

            q.prefill_done += n_tokens

            # Check if prefill completed
            if q.prefill_done >= q.L and q.ttft_time is None:
                q.ttft_time = t

                if q.id >= warmup_id:
                    metrics["ttft"].append(q.ttft_time - q.arrival_time)

                if q not in decode_queue:
                    decode_queue.append(q)

        # Decode phase update
        elif phase == "decode":

            q.decode_done += n_tokens

            # Compute per-token TBT
            if q.id >= warmup_id:
                if q.last_decode_finish is not None:
                    metrics["tbt"].append(t - q.last_decode_finish)
                    metrics["tbt_time"].append(t)
                q.last_decode_finish = t

            # If decode finishes, record completion time
            if q.decode_done >= q.B and q.completion_time is None:
                q.completion_time = t

                if q.id >= warmup_id:
                    metrics["completion"].append(q.completion_time - q.arrival_time)



    # Clean up queues
    prefill_queue[:] = [q for q in prefill_queue if q.prefill_done < q.L]

    decode_queue[:] = [
        q for q in decode_queue
        if q.prefill_done >= q.L and q.decode_done < q.B
    ]

def simulate(
    policy: Policy,
    *,
    n_queries: int = 4000,      # total number of queries the simulator will process
    warmup: int = 500,          # number of initial queries ignored in metrics
    lam: float = 1.0,         # average number query arrivals (Poisson process)
    L: int = 64,                # prefill tokens per query
    B: int = 32,                # decode tokens per query
    c_ms: float = 40.0,         # service model constant term (ms)
    a_ms: float = 0.3,          # service model slope term (ms/token)
    b0: int = 64,               # breakpoint for service model
    K: int = 128,               # batch token capacity
    seed: int = 0,
) -> Dict[str, Any]:

    rng = np.random.default_rng(seed)

    # Convert ms to seconds for service model
    c = c_ms / 1000.0
    a = a_ms / 1000.0

    # Current simulation time
    t = 0.0

    # Time of next arrival event
    next_arrival = rng.exponential(1 / lam)

    # GPU state
    gpu_busy_until: Optional[float] = None
    active_batch: Batch = []

    # Queues
    prefill_queue: List[Query] = []
    decode_queue: List[Query] = []
    all_queries: List[Query] = []

    # Metrics
    metrics = {
        "ttft": [],
        "ttft_time": [],
        "tbt": [],
        "tbt_time": [],
        "completion": [],
        "first_decode_times": []
    }


    # Completions counting for steady-state
    completed_after_warmup = 0
    target_completed = n_queries - warmup

    # Helper: dispatch a new batch if GPU is idle
    def try_dispatch():
        nonlocal active_batch, gpu_busy_until
        if gpu_busy_until is None:
            batch, finish_time = policy.dispatch_batch(
                t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K
            )
            if batch:
                active_batch[:] = batch
                gpu_busy_until = finish_time

    # Main event loop
    while completed_after_warmup < target_completed:

        # Next arrival vs next GPU completion
        candidate_times = [next_arrival]
        if gpu_busy_until is not None:
            candidate_times.append(gpu_busy_until)

        # Advance simulation time
        t_next = min(candidate_times)
        t = t_next

        # EVENT TYPE 1: arrival
        if t == next_arrival:
            q = Query(id=len(all_queries), arrival_time=t, L=L, B=B)
            all_queries.append(q)
            prefill_queue.append(q)

            next_arrival = t + rng.exponential(1 / lam)

            try_dispatch()

        # EVENT TYPE 2: Batch Completion
        else:
            gpu_busy_until = None

            before = len(metrics["completion"])

            apply_batch_completion(
                t,
                active_batch,
                prefill_queue,
                decode_queue,
                warmup_id=warmup,
                metrics=metrics,
            )

            after = len(metrics["completion"])
            completed_after_warmup += (after - before)

            active_batch = []

            try_dispatch()

    # Throughput estimation
    post = [q for q in all_queries if q.id >= warmup and q.completion_time is not None]
    post.sort(key=lambda q: q.completion_time)

    if len(post) >= 2:
        elapsed = post[-1].completion_time - post[0].arrival_time
        throughput = len(post) / elapsed if elapsed > 0 else float("nan")
    else:
        throughput = float("nan")


    # Return metrics
    return {
        "throughput": throughput,
        "ttft": np.array(metrics["ttft"]),
        "ttft_time": np.array(metrics["ttft_time"]),
        "tbt": np.array(metrics["tbt"]),
        "tbt_time": np.array(metrics["tbt_time"]),
        "completion": np.array(metrics["completion"]),
    }

def run_demo():
    """
    Run a simple comparative demo between two batching policies:
      • Serve-to-Completion
      • Prefill-First
    """

    # Two policies to compare
    baseline = ServeToCompletionPolicy()
    prefill_first = PrefillFirstPolicy()
    hybrid = HybridSchedulerPolicy(Q=25)

    # Run simulation under each policy
    # Uses the same seed for both simulations
    r1 = simulate(baseline, seed=1)
    r2 = simulate(prefill_first, seed=1)
    r3 = simulate(hybrid, seed=1)

    # Print throughput comparison
    print("Serve-to-completion throughput:", r1["throughput"])
    print("Prefill-first throughput:", r2["throughput"])
    print("Hybrid throughput:", r3["throughput"])

    # Print TTFT
    print("Mean TTFT baseline:", r1["ttft"].mean())
    print("Mean TTFT prefill-first:", r2["ttft"].mean())
    print("Mean TTFT hybrid:", r3["ttft"].mean())

    # Print token-level TBT
    # TBT may be empty depending on workload,so guard with a length check.
    print("Mean token-level TBT baseline:",
          r1["tbt"].mean() if len(r1["tbt"]) else np.nan)

    print("Mean token-level TBT prefill-first:",
          r2["tbt"].mean() if len(r2["tbt"]) else np.nan)

    print("Mean token-level TBT hybrid:",
          r3["tbt"].mean() if len(r3["tbt"]) else np.nan)


    # Return raw results for further analysis and plotting
    return r1, r2, r3

def plot_demo_hist(r1, r2, r3, bin_width=100.0):
    """
    Plot histograms of mean TTFT and mean TBT within fixed time bins,
    now including the Hybrid scheduler.
    """

    # Helper: compute histogram of means
    def mean_hist(times, values, bin_width):
        if len(times) == 0:
            return np.array([]), np.array([])

        t_min = times.min()
        t_max = times.max()

        bins = np.arange(t_min, t_max + bin_width, bin_width)

        # Digitize into bins
        idx = np.digitize(times, bins) - 1

        bin_means = []
        bin_centers = []

        for b in range(len(bins) - 1):
            mask = (idx == b)
            if mask.any():
                bin_means.append(values[mask].mean())
                bin_centers.append((bins[b] + bins[b+1]) / 2)

        return np.array(bin_centers), np.array(bin_means)

    # TTFT histograms (by query index)
    plt.figure(figsize=(8,5))

    # Query indices
    ttft_t1 = np.arange(len(r1["ttft"]))
    ttft_t2 = np.arange(len(r2["ttft"]))
    ttft_t3 = np.arange(len(r3["ttft"]))

    c1, m1 = mean_hist(ttft_t1, r1["ttft"], bin_width)
    c2, m2 = mean_hist(ttft_t2, r2["ttft"], bin_width)
    c3, m3 = mean_hist(ttft_t3, r3["ttft"], bin_width)

    offset = bin_width * 0.2
    width = bin_width * 0.25

    plt.bar(c1 - offset, m1, width=width, label="TTFT baseline")
    plt.bar(c2, m2, width=width, label="TTFT prefill-first")
    plt.bar(c3 + offset, m3, width=width, label="TTFT hybrid")

    plt.xlabel("Query index (binned)")
    plt.ylabel("Mean TTFT (s)")
    plt.title("Mean TTFT per query (binned)")
    plt.legend()
    plt.show()

    # TBT histograms (time-based)
    plt.figure(figsize=(8,5))

    c1, m1 = mean_hist(r1["tbt_time"], r1["tbt"], bin_width)
    c2, m2 = mean_hist(r2["tbt_time"], r2["tbt"], bin_width)
    c3, m3 = mean_hist(r3["tbt_time"], r3["tbt"], bin_width)

    plt.bar(c1 - offset, m1, width=width, label="TBT baseline")
    plt.bar(c2, m2, width=width, label="TBT prefill-first")
    plt.bar(c3 + offset, m3, width=width, label="TBT hybrid")

    plt.xlabel("Time (s, binned)")
    plt.ylabel("Mean TBT (s)")
    plt.title("Mean TBT in time bins")
    plt.legend()
    plt.show()

#if __name__ == "__main__":
    #r1, r2, r3 = run_demo()
    #plot_demo_hist(r1,r2,r3)

import random
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional

# Mock Query class for demonstration
class Query:
    def __init__(self, prefill_tokens: int):
        self.prefill_tokens = prefill_tokens

    def remaining_prefill(self):
        return self.prefill_tokens

    def consume_tokens(self, n: int):
        self.prefill_tokens -= n

BatchItem = Tuple[Query, int, str]
Batch = List[BatchItem]

# Define policies

class FixedBatchPolicy:
    def dispatch_batch(self, t, prefill_queue, decode_queue, *, c, a, b0, K):
        batch: Batch = []
        remaining = K
        for q in prefill_queue:
            if remaining <= 0:
                break
            tokens = min(q.remaining_prefill(), remaining)
            batch.append((q, tokens, "prefill"))
            remaining -= tokens
        return batch, None

class DynamicBatchPolicy:
    def dispatch_batch(self, t, prefill_queue, decode_queue, *, c, a, b0, K):
        batch: Batch = []
        remaining = K
        # pick only a few queries instead of filling K
        for q in prefill_queue[:3]:
            if remaining <= 0:
                break
            tokens = min(q.remaining_prefill(), remaining)
            batch.append((q, tokens, "prefill"))
            remaining -= tokens
        return batch, None

class NoBatchPolicy:
    def dispatch_batch(self, t, prefill_queue, decode_queue, *, c, a, b0, K):
        batch: Batch = []
        if prefill_queue:
            q = prefill_queue[0]
            batch.append((q, 1, "prefill"))  # always 1 token
        return batch, None

# Simulation function

def simulate(policy, num_queries=50, max_tokens_per_query=10, K=16):
    batch_sizes = []
    # generate queries with random prefill tokens
    prefill_queue = [Query(random.randint(1, max_tokens_per_query)) for _ in range(num_queries)]
    decode_queue = []  # unused in this simple demo

    t = 0.0
    while prefill_queue:
        batch, _ = policy.dispatch_batch(t, prefill_queue, decode_queue, c=0, a=0, b0=0, K=K)
        if not batch:
            break
        batch_size = sum(n for _, n, _ in batch)
        batch_sizes.append(batch_size)
        # consume tokens
        for q, n, _ in batch:
            q.consume_tokens(n)
        # remove completed queries
        prefill_queue = [q for q in prefill_queue if q.remaining_prefill() > 0]

    return batch_sizes

# Run simulations

K = 32
batch_sizes_fixed = simulate(FixedBatchPolicy(), K=K)
batch_sizes_dynamic = simulate(DynamicBatchPolicy(), K=K)
batch_sizes_nobatch = simulate(NoBatchPolicy(), K=K)

# Plot Figure 1

plt.figure(figsize=(8,5))
plt.hist(batch_sizes_nobatch, bins=range(1,K+2), alpha=0.5, label='No batching', rwidth=0.8)
plt.hist(batch_sizes_fixed, bins=range(1,K+2), alpha=0.5, label='Fixed batching', rwidth=0.8)
plt.hist(batch_sizes_dynamic, bins=range(1,K+2), alpha=0.5, label='Dynamic batching', rwidth=0.8)

plt.xlabel('Batch Size (tokens)')
plt.ylabel('Frequency')
plt.title('Figure 1: Batch size distributions under three strategies')
plt.legend()
plt.show()

# Modeling Throughput

def simulate_throughput(policy, num_queries=50, max_tokens_per_query=10, K=16, c=0.01, a=0.001, b0=8):
    total_tokens = 0
    total_time = 0.0
    prefill_queue = [Query(random.randint(1, max_tokens_per_query)) for _ in range(num_queries)]
    decode_queue = []

    t = 0.0
    while prefill_queue:
        batch, _ = policy.dispatch_batch(t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K)
        if not batch:
            break

        # batch size
        b = sum(n for _, n, _ in batch)
        total_tokens += b

        # compute service time S(b)
        batch_time = c + a * max(0, b - b0)
        total_time += batch_time

        # consume tokens
        for q, n, _ in batch:
            q.consume_tokens(n)
        prefill_queue = [q for q in prefill_queue if q.remaining_prefill() > 0]

    throughput = total_tokens / total_time if total_time > 0 else 0.0
    return throughput

# Run simulations

K = 16
throughput_nobatch = simulate_throughput(NoBatchPolicy(), K=K)
throughput_fixed = simulate_throughput(FixedBatchPolicy(), K=K)
throughput_dynamic = simulate_throughput(DynamicBatchPolicy(), K=K)

# Plot Figure 2

strategies = ['No batching', 'Dynamic batching', 'Fixed batching']
throughputs = [throughput_nobatch, throughput_dynamic, throughput_fixed]

plt.figure(figsize=(8,5))
plt.bar(strategies, throughputs, color=['gray', 'blue', 'green'], alpha=0.7)
plt.ylabel('Throughput (tokens/sec)')
plt.title('Figure 2: Throughput under different batching strategies')
plt.show()

class Query:
    def __init__(self, prefill_tokens: int):
        self.prefill_tokens = prefill_tokens
        self.first_token_time: float = None  # record TTFT
        self.token_times: List[float] = []   # record TBT

    def remaining_prefill(self):
        return self.prefill_tokens

    # Updated to accept time 't'
    def consume_tokens(self, n: int, t: float):
        if self.first_token_time is None:
            self.first_token_time = t
        # record time for each token in this batch
        for _ in range(n):
            self.token_times.append(t)
        self.prefill_tokens -= n


def simulate_ttf_tbt(policy, num_queries=50, max_tokens=10, K=16, c=0.01, a=0.001, b0=8):
    t = 0.0
    # create all queries
    all_queries = [Query(random.randint(1, max_tokens)) for _ in range(num_queries)]
    prefill_queue = all_queries.copy()
    decode_queue = []

    while prefill_queue:
        batch, _ = policy.dispatch_batch(t, prefill_queue, decode_queue, c=c, a=a, b0=b0, K=K)
        if not batch:
            break

        # total tokens in batch
        b = sum(n for _, n, _ in batch)
        batch_time = c + a * max(0, b - b0)

        # record times for TTFT and TBT
        for q, n, _ in batch:
            q.consume_tokens(n, t)

        t += batch_time
        # remove completed queries
        prefill_queue = [q for q in prefill_queue if q.remaining_prefill() > 0]

    # compute TTFT and TBT statistics using all queries
    ttft_list = [q.first_token_time for q in all_queries if q.first_token_time is not None]
    tbt_list = [max(q.token_times) - min(q.token_times) for q in all_queries if q.token_times]

    return ttft_list, tbt_list

# Run simulations for all strategies

K = 16
policies = {
    "No batching": NoBatchPolicy(),
    "Dynamic batching": DynamicBatchPolicy(),
    "Fixed batching": FixedBatchPolicy()
}

ttft_stats = {}
tbt_stats = {}

for name, policy in policies.items():
    ttft_list, tbt_list = simulate_ttf_tbt(policy, K=K)
    ttft_stats[name] = {
        'mean': sum(ttft_list)/len(ttft_list),
        'p95': sorted(ttft_list)[int(0.95*len(ttft_list))]
    }
    tbt_stats[name] = tbt_list

# Figure 3: TTFT mean and P95

labels = list(ttft_stats.keys())
mean_ttft = [ttft_stats[l]['mean'] for l in labels]
p95_ttft = [ttft_stats[l]['p95'] for l in labels]

x = range(len(labels))
plt.figure(figsize=(8,5))
plt.bar(x, mean_ttft, width=0.4, label='Mean TTFT', alpha=0.7)
plt.bar([i+0.4 for i in x], p95_ttft, width=0.4, label='P95 TTFT', alpha=0.7)
plt.xticks([i+0.2 for i in x], labels)
plt.ylabel('Time (sec)')
plt.title('Figure 3: Mean and 95th Percentile TTFT per batching strategy')
plt.legend()
plt.show()

# Figure 4: TBT distributions

plt.figure(figsize=(8,5))
for name in labels:
    plt.hist(tbt_stats[name], bins=20, alpha=0.5, label=name)
plt.xlabel('TBT (sec)')
plt.ylabel('Frequency')
plt.title('Figure 4: Distribution of TBT under batching strategies')
plt.legend()
plt.show()